# 进程和线程的区别和联系

**进程：**进程是系统中正在运行的程序。是计算机分配资源的单位，每一个进程都会有属于自己独立的内存空间，磁盘空间，I\O设备等等。比如说windows中的任务管理器，每一行都是一个进程。

**线程：**线程是进程的一部分，之前看到书上说线程是任务调度的基本单元。（现在来看这局话其实就是正在运行的一个程序有很多任务，而线程是将任务呈现出细粒度，更精确了）其实我觉得线程主要扮演的角色就是如何利用cpu处理代码，完成当前进程中的各个子任务。各个子线程之间共享父进程的代码空间和全局变量，但是每个进程都有自己独立的堆栈，即局部变量对于线程来说是私有的。因此创建多进程代价有点大，在一个进程中创建多线程代价要小很多。

做个简单的比喻：进程=火车，线程=车厢

1. 线程在进程下行进（单纯的车厢无法运行）
2. 一个进程可以包含多个线程（一辆火车可以有多个车厢）
3. 不同进程间数据很难共享（一辆火车上的乘客很难换到另外一辆火车，比如站点换乘）
4. 同一进程下不同线程间数据很易共享（A车厢换到B车厢很容易）
5. 进程要比线程消耗更多的计算机资源（采用多列火车相比多个车厢更耗资源）
6. 进程间不会相互影响，一个线程挂掉将导致整个进程挂掉（不一定）（一列火车不会影响到另外一列火车，但是如果一列火车上中间的一节车厢着火了，将影响到所有车厢）
7. 线程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。（比如火车上的洗手间）－"互斥锁"

>  所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。所以，对于线程和进程，我们可以这么理解： - 当进程只有一个线程时，可以认为进程就等于线程。 - 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。 - 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。



# 进程间通信方式

每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，内核是可以共享的。在内核中开辟一块缓冲区，进程1把数据从用户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为**进程间通信（IPC，InterProcess Communication）**

- **管道**

  管道允许进程以先进先出的方式传送数据，是半双工的，意味着数据只能往一个方向流动。因此当双方通信时，必须建立两个管道。

  管道的实质就是在内核中创建一个缓冲区，管道一端的进程进入管道写数据，另一端的进程进入管道读取数据。

  管道分为pipe和FIFO两种

  - pipe：用于相关联的进程，比如父进程和子进程之间的通信。
  - fifo：命名管道，即任何进程可以根据管道的文件名将其打开和读写。

  缺点：管道本质上是通过内核交换数据的，因此通信效率很低，不适合频繁交换数据的情况。

  匿名管道的周期随着进程的创建而创建，销毁而销毁。

- **消息队列**

  消息队列是保存在内核中的链表，由一个个独立的数据块组成，消息的接收方和发送方要约定具体的消息类型。当进程从消息队列中读取了相关数据块，则内核会将该数据块删除。跟管道相比，消息队列不一定按照先进先出的方式读取，也可以按照消息类型进行兑取。

  消息队列的生命周期与内核相关，如果不显示的删除消息队列，则消息队列会一直存在

  消息队列这种通信方式，跟收发邮件类似。两个进程你来我往的进行沟通

  缺点：不能实现实时通信。数据块是有大小限制的。**消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销**，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。

- **共享内存**

  共享内存技术就是要解决用户态和内核态之间频繁发生拷贝过程的。现代操作系统对于内存管理普遍采用的是虚拟内存技术，每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存空间映射到不同的物理内存中。

  数据不需要在不同进程之间进行复制，这是最快的一种IPC

  共享内存技术的实质就是拿出一块虚拟地址空间，映射到相同的物理内存中。这样的好处是一个进程写入数据后另一个进程可以立刻看到，不用进行拷贝。效率很高。

- **信号量**（同步原语，并不叫IPC）

  **本质是一个计数器，用于为多个进程提供对共享对象的访问**

  共享内存在效率高的同时也带来了新的问题，即如果多个进程同时对一个共享内存进行操作，会产生冲突造成不可预计的后果。

  为了不冲突，共享内存在一个时间段只能有一个进程访问，就出现了信号量。

  信号量其实是一个计数器，用于实现进程间的互斥和同步。

  **信号量表示资源的数量。** **P操作** 会把信号量-1，-1之后如果信号量的值<0，则表示资源已经被占用，进程需要阻塞等待。如果信号量-1后>= 0，表明进程可以正常执行。**V操作**跟P操作正好相反。

  <img src="https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/20210819115755.png" alt="img" style="zoom:50%;" />

  1. 进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。

  2. 若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。

  3. 直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。

     > **信号量与互斥量之间的区别：**
     >
     > （1）互斥量用于线程的互斥，信号量用于线程的同步。这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。
     >
     > **互斥：**是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。
     >
     > **同步：**是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。
     >
     > （2）互斥量的加锁和解锁必须由同一线程分别对应使用，信号量可以由一个线程释放，另一个线程得到。

- **信号**

  上面那几种进程间通信，都是常规状态下的。异常状态下的需要用信号来通知进程。

  信号和信号量就像雷锋和雷峰塔的区别。

  可以在任何时刻给进程发送信号，信号是进程间通信或操作的一种异步通信机制。

  收到信号后进程对信号的处理有三种方式

  1. 如果是系统定义的信号函数，执行默认操作。

     >**SIGINT：**程序终止信号。程序运行过程中，按`Ctrl+C`键将产生该信号。
     >
     >**SIGQUIT：**程序退出信号。程序运行过程中，按`Ctrl+\\`键将产生该信号。
     >
     >**SIGALRM：**定时器信号。
     >
     >**SIGTERM：**结束进程信号。shell下执行`kill 进程pid`发送该信号。

  2. 捕捉信号。用户可以给信号定义信号处理函数，表示收到信号后该进程该怎么做。

  3. 忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SEGSTOP`，它们用于在任何时候中断或结束某一进程。

- **unix域套接字**

  具体指unix域间套接字。socket API原本是为网络通讯设计的，但后来在socket的框架上发展出一种IPC机制，就是UNIX Domain Socket。虽然网络socket也可用于同一台主机的进程间通讯（通过loopback地址127.0.0.1），但是UNIX Domain Socket用于IPC更有效率：不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号和应答等，只是将应用层数据从一个进程拷贝到另一个进程。**UNIX域套接字与TCP套接字相比较，在同一台主机的传输速度前者是后者的两倍。**这是因为，IPC机制本质上是可靠的通讯，而网络协议是为不可靠的通讯设计的。UNIX Domain Socket也提供面向流和面向数据包两种API接口，类似于TCP和UDP，但是面向消息的UNIX Domain Socket也是可靠的，消息既不会丢失也不会顺序错乱。



# 线程间通信的方式

线程间很多资源都是共享的，所以线程间没有像进程间那样有许多数据交换机制。**线程间通信主要目的是为了线程同步。**

以下知识点为扩散，哪些是共享资源，哪些不是。

> 同一进程的所有线程共享以下资源：
>
> 1. 堆。堆是在进程空间内开辟的，所以被共享
> 2. 全局变量。与某一函数无关，与特定线程无关
> 3. 静态变量。静态变量存放位置和全局变量一样，都存在于堆中开辟的.bss和.data段，是共享的
> 4. 其他一些共用资源，比如文件。
>
> 同一进程的所有线程独享以下资源：
>
> 1. 栈。
> 2. 寄存器。
> 2. 程序计数器
>
> **线程运行的本质就是函数的执行**，而函数的执行总会有一个源头，这个源头叫做入口函数，cup从入口函数开始一步一步向下执行，这个过程就叫做线程。由于函数运行时信息是保存在栈中的，比如返回值，参数，局部变量等等，所以栈是私有的。
>
> cpu执行指令的信息会保存在寄存器中，这个寄存器叫做程序计数器。由于操作系统可以随时终止线程的运行，所以保存和恢复程序计数器的值就知道线程从哪里暂停的以及从哪里开始运行。

- **锁机制**

  - 互斥锁。确保同一时间内只有一个线程能访问共享资源。当资源被占用时其他试图加锁的线程会进入阻塞状态，。当锁释放后，哪个线程能上锁取决于内核调度。
  - 读写锁。当以写模式加锁的时候，任何其他线程不论以何种方式加锁都会处以阻塞状态。当以读模式加锁时，读状态不阻塞，但是写状态阻塞。“读模式共享，写模式互斥”
  - 自旋锁。上锁受阻时线程不阻塞而是在循环中轮询查看能否获得该锁，没有线程的切换因而没有切换开销，不过对CPU的霸占会导致CPU资源的浪费。
- **posix信号量机制**

  信号量本质上是一个计数器，可以有PV操作，来控制多个进程或者线程对共享资源的访问。

  信号量API有两组，第一组就是System V IPC信号量用于进程间通信的，另外一组就是POSIX信号量，信号量原理都是一样的

- **条件变量**

  条件变量提供了线程间的通知机制：当某个共享数据到达某个值的时候，唤醒等待这个共享数据的线程。

  条件变量要结合互斥锁使用，如下图代码：

  ```c
  pthread_mutex_lock(&mutex);　
  　　while(条件为假)
  　　　　pthread_cond_wait(&cond,&mutex);　　
  　　执行某种操作
  pthread_mutex_unlock(&mutex);
  ```

  也就是说，一个线程要等到临界区的共享数据达到某种状态时再进行某种操作，而这个状态的成立，则是由另外一个进程/线程来完成后发送信号来通知的。



# linux内核的同步方式

首先要明确，同步和互斥是计算机系统中，用于控制进程对某些特定资源访问的机制。同步指的是进程按照一定的顺序和规则访问资源，互斥则是控制资源某一时刻只能由一个进程访问。这样看来互斥是同步的一种情况。

在现代操作系统里，同一时间可能有多个内核执行流在执行，因此内核其实象多进程多线程编程一样也需要一些同步机制来同步各执行单元对共享数据的访问。尤其是在多处理器系统上，更需要一些同步机制来同步不同处理器上的执行单元对共享的数据的访问。在主流的Linux内核中包含了几乎所有现代的操作系统具有的同步机制，**这些同步机制包括：原子操作、信号量（semaphore）、读写信号量 （rw_semaphore）、spinlock、BKL(Big Kernel Lock)、rwlock、brlock（只包含在2.4内核中）、RCU（只包含在2.6内核中）和seqlock（只包含在2.6内核中）。**

> linux会因为三种机制而产生并发，需要进行同步。
>
> 1. **中断。**中断就是操作系统随时可以打断正在执行的代码。当进程访问某个临界资源发生中断，会进入中断处理程序。中断处理程序也可能访问该临界资源。
> 2. **内核抢占式调度。**内核中正在执行的任务被另外一个任务抢占。
> 3. **多处理器并发。**每个处理器都可以调度一个进程，多个进程可能会造成并发。

- **禁用中断**

  对于单处理器不可抢占系统来说，系统并发源主要是中断处理。因此在进行临界资源访问时，进行禁用/使能中断即可以达到消除异步并发源的目的。

- **原子操作**

  原子操作保证指令在运行时候不会被任何事物或者事件打断，把读和写的行为包含在一步中执行，避免竞争。

- **内存屏障(memory barrier)**

  程序在运行时内存实际的访问顺序和程序代码编写的访问顺序不一定一致，这就是内存乱序访问。内存乱序访问行为出现的理由是为了提升程序运行时的性能。内存乱序访问主要发生在两个阶段：

  1. 编译时，编译器优化导致内存乱序访问（指令重排）
  2. 运行时，多 CPU 间交互引起内存乱序访问

- **自旋锁(spin lock)**

  对于复杂的操作原子操作是不能的。比如从一种数据结构中读取数据，写入到另一种数据结构中。自旋锁是linux内核中最常见的一种锁。自旋锁只能被一个可执行线程所拥有，如果有线程要抢占一个已经被占有的自旋锁，则其会一直进行循环来等待锁重新可用。当锁被释放后，请求锁的执行线程便能立刻得到它。**就是忙等**

  **自旋锁只适用于那些在临界区停留很短时间的加锁操作。**因为线程在等待锁期间会一直占据处理器，如果长时间等待锁会导致处理器效率降低。而如果线程占用锁只需要短暂的执行一下，那么使用自旋锁更优，因为不需要进行上下文的切换。

- **信号量**

  可以理解为一种“睡眠锁”

  自旋锁是“忙等”机制，在临界资源被锁定的时间很短的情况下很有效。但是在临界资源被持有时间很长或者不确定的情况下，忙等机制则会浪费很多宝贵的处理器时间。

  而信号量机制在进程无法获取到临界资源的情况下，立即释放处理器的使用权，并睡眠在所访问的临界资源上对应的等待队列上；在临界资源被释放时，再唤醒阻塞在该临界资源上的进程。信号量适用于长时间占用锁的情形。

- **读-写自旋锁**

  具体就是在读写场景中了，为读和写提供不同的锁机制。

  当一个或者多个读任务时可以并发的持有读锁，但是写锁只能被一个人物所持有的。即对写者共享对读者排斥

- **读写信号量**

  这个和读写自旋锁的思想是一样的。

- **mutex体制**

  也是一种睡眠锁，是实现互斥的特定睡眠锁。是一种互斥信号

  使用mutex有很多限制，不像信号量那样想用就用

  1. 任何时刻只有一个任务可以持有mutex，引用计数只能是1
  2. 给mutex上锁必须给其解锁，严格点就是必须在同一上下文中上锁和解锁
  3. 持有mutex的进程不能退出
  4. 等等

- **完成变量(completion variable)**

  内核中一个任务需要发出信号通知另一个任务发生了某个特定事件，利用完成变量使得两个任务实现同步

- **BLK：大内核锁**

  属于混沌时期的产物，是一个全局的自旋锁

  这个东西是早期linux不支持线程的时候用的，和自旋锁差不多的思想。

  现在一般不鼓励使用这个了

- **顺序锁**

  这种锁用于读写共享数据。

  实现这个机制主要依靠序列计数器，当写数据时，会得到一个锁，序列值会增加。在读取数据前后这两个时间内，序列值都会被读取

  如果读取的序列号值相同，表明读操作在进行的过程中没有被写操作打断，

- **关闭内核抢占**

  内核是抢占性的，因此内核中的进程在任何时候都可能停下来以便让另一个具有更高优先级的进程运行。但是一个任务和被抢占的任务可能在同一个临界区运行。为了避免上述情况，当使用自旋锁时这个区域被标记为非抢占的，一个自旋锁被持有表示内核不能抢占调度。

  但是在一些情况下，就算不用自旋锁，也要关闭内核抢占。

  比如，对于只有一个处理器能够访问到数据，原理上是没有必要加自旋锁的，因为在任何时刻数据的访问者永远只有一位。但是，如果内核抢占没有关闭，则可能一个新调度的任务就可能访问同一个变量。
  
  所以这时候害怕的不是多个任务访问同问同一个变量，而是一个任务的访问还没有完成就转到了另一个任务。



# 死锁

  > 概念：由于操作系统会产生并发，那会产生一个问题，就是多个进程因为争夺资源而互相陷入等待。

  - 死锁产生的原因

    1. 资源分配不当
    2. 进程运行的顺序不合理

  - 产生死锁的**必要条件**

    1. 互斥。某个资源只允许一个进程访问，如果已经有进程访问该资源，则其他进程就不能访问，直到该进程访问结束。
    2. 占有的同时等待。一个进程占有其他资源的同时，还有资源未得到，需要其他进程释放该资源。
    3. 不可抢占。别的进程已经占有某资源，自己不能去抢。
    4. 循环等待。存在一个链条，每个进程都需要下一个进程的资源。

    以上四个条件均满足必然会造成死锁。会导致cpu吞吐量下降，死锁会浪费系统资源造成计算机性能下降。

  - 避免死锁的方法。

    我们要尽量避免四个条件同时产生，因此就要破坏。由于互斥条件是必须的，必须要保证的，因此从后面三条下手。

    1. 破坏“占有且等待条件”。

       ①所有进程在开始运行之前，一次性申请到所有所需要的资源。

       ②进程用完的资源释放掉，然后再去请求新的资源，提高利用率。

    2. 破坏“不可抢占”条件。

       当进程提出在得到一些资源时候不被满足的情况下，必须释放自己已经保存的资源。

    3. 破坏“循环等待”。

       实现资源有序分配策略，所有进程申请资源必须按照顺序执行。

    4. 银行家算法
    
       [参考链接](https://www.cnblogs.com/wkfvawl/p/11929508.html)



# 虚拟内存是如何映射到物理内存的？

> 这个在我关于操作系统那本书的总结中有，可以去看一下

程序在编译时不可能知道装载后的物理内存地址，实际上，程序编译生成的地址都是虚拟地址。在我们日常使用的 Linux 或者 Windows 操作系统下，程序并不能直接访问物理内存。为了解决这个问题，当程序装载后，会通过虚拟地址映射到真实的物理地址。

内存被分成固定大小的**页(Page)**，然后再通过**虚拟内存地址(Virtual Address)** 到**物理内存地址(Physical Address)** 的**地址转换(Address Translation)**，才能访问实际存放数据的物理内存位置。而我们的程序看到的内存地址，都是虚拟内存地址。

**页表(Page Table)**：想要把虚拟内存地址，映射到物理内存地址，最直观的办法，就是来建一张映射表。这个映射表，能够实现虚拟内存里面的页，到物理内存里面的页的映射。这个映射表，在计算机里面，就叫作页表。

- 简单页表

  页表地址转换，把一个内存地址分成**页号(Directory)** 和**偏移量(Offset)** 两个部分。、

  <img src="https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/20210903091805.png" alt="img" style="zoom: 25%;" />

  对于一个内存地址转换，其实就是这样三个步骤：

  1. 把虚拟内存地址，切分成页号和偏移量的组合；
  2. 从页表里面，查询出虚拟页号，对应的物理页号；
  3. 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

- 多级页表（页表树）

  简单页表的实现很耗费存储空间。大部分进程所占用的内存是有限的，我们只需要保存那些用到的页之间的映射关系就好了。

  **在整个进程的内存地址空间，通常是 "两头实、中间空"。在程序运行的时候，内存地址从顶部往下，不断分配占用的栈的空间。而堆的空间，内存地址则是从底部往上，是不断分配占用的。**所以，在一个实际的程序进程里面，虚拟内存占用的地址空间，通常是两段连续的空间。而不是完全散落的随机的内存地址。而多级页表，就特别适合这样的内存地址分布。

  <img src="https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/20210903092546.jpeg" alt="img" style="zoom: 25%;" />

  <img src="https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/20210903092743.png" alt="img" style="zoom:25%;" />

  跟B+树类似，允许一个结点存储多条记录，并且非叶子结点只存储索引，只有叶子结点存储数据。

  多级页表采用时间换空间的做法，原本我们访问一次就能找到物理页号，现在 要访问树的高度次



# 说一说内存对齐

- 由一个例子引出内存对齐

  ```c
  //32位系统
  #include<stdio.h>
  struct{
      int x;
      char y;
  }s;
  
  int main()
  {
      printf("%d\n",sizeof(s);  // 输出8
      return 0;
  }
  ```

  上述代码实际得到的值是8byte，这就是内存对齐所导致的。

  > 从理论上来讲，任何类型的变量可以从任意地址开始存放。但是实际上计算机对基本数据类型在内存中的存放位置有限制，会要求数据首地址的值是某个数（通常是4或8）的整数倍，这就是内存对齐。

- 为什么要内存对齐呢？

  计算机的内存是以字节为单位的，但是大部分处理器并不是按照字节块来存取内存的。计算机有个内存存取粒度的概念，就是说一般以2字节，4字节，8字节这样的单位来存取内存。

  加入没有内存对齐机制，数据可以任意存放。地址排列为01234567现在读取一个int型变量，这个变量存放地址从1开始，那我们从0字节开始找，找到1后读取第一个4字节，把0扔掉，然后从地址4开始读下4个字节，扔掉567地址，int变量就存储在1234这里，可以看到这样很浪费开销，因为访问了两次内存。

- 内存对齐的规则

  对其规则如下：

  1. 基本类型的对齐值就是sizeof值
  2. 结构体。结构体本身也要对齐，按照最大成员长度来参照的。
  3. 编译器可以设置最大对齐值，gcc中默认是#pragma pack(4)。但是类型的实际对齐值与默认对齐值取最小值来



# 内存空间的堆和栈的区别是什么？

- 程序内存布局场景下，堆与栈表示两种内存管理方式；

  - 栈是由操作系统自动分配的，用于存放函数参数值，局部变量。存储在栈中的数据的生命周期随着函数的执行结束而结束。栈的内存生长方向与堆相反，由高到低，按照变量定义的先后顺序入栈。

  - 堆是由用户自己分配的。如果用户不回收，程序结束后由操作系统自动回收。堆的内存地址生长方向与栈相反，由低到高。

    > **堆上分配内存的过程：**
    >
    > 操作系统有一个记录空闲内存地址的链表，当系统收到程序的开辟内存申请时候，会遍历该链表，寻找第一个空间大于所申请内存空间的节点。接着把该节点从空闲链表中删除，同时将该空间分配出去给程序使用。同时大多数系统会在内存空间中的首地址记录此次分配的大小，这样delete才能正确释放内存空间。由于找到的节点所对应的内存大小不一定正好等于申请内存的大小，OS会自动的将多余的部分放入空闲链表。

  - 堆与栈区别的总结

    1. 管理方式不同
    2. 空间大小不同
    3. 分配方式不同。堆都是动态分配。栈的静态分配由操作系统完成，回收也是自动的。栈的动态分配有alloca函数分配，由操作系统自动回收。
    4. 分配效率不同。栈由操作系统自动分配，会在硬件层级对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高。堆则是由C/C++提供的库函数或运算符来完成申请与管理，实现机制较为复杂，频繁的内存申请容易产生内存碎片。显然，堆的效率比栈要低得多。

- 数据结构场景下，堆与栈表示两种常用的数据结构。

  栈是线性结构

  堆是一种特殊的完全二叉树



# 进程虚拟空间是怎么布局的？进程内存模型

linux进程在32位处理器下的虚拟空间内存布局，从高地址到低地址

![img](https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/20210831210023.png)

- 内核空间，从C000000-FFFFFFFF

- 栈区。有以下用途

  1. 存储函数局部变量
  2. 记录函数调用过程的相关信息，成为栈帧，主要包括函数返回地址，一些不适合放在寄存器中的函数参数。
  3. 临时存储区，暂存算术表达式的计算结果和allocation函数分配的栈内存。

- 内存映射段（mmap）。内核将硬盘文件的内容直接映射到内存，是一种方便高校的文件I/O方式，用于装在动态共享库。

- 堆。分配的堆内存是经过字节对齐的空间，以适合原子操作。堆管理器通过链表管理每个申请的内存，由于堆申请和释放是无序的，最终会产生内存碎片。堆内存一般由应用程序分配释放，回收的内存可供重新使用。若程序员不释放，程序结束时操作系统可能会自动回收。

- BSS段。通常存放以下内容：

  1. 未初始化的全局变量和静态局部变量
  2. 初始化值为0的全局变量和静态局部变量
  3. 未定义且初值不为0的符号

- 数据段。通常存放程序中已经初始化且初值不为0的全局变量。数据段属于静态存储区，可读可写。

  数据段和BSS段的区别如下：

  1. BSS段不占用物理文件尺寸，但占用内存空间（不在可执行文件中）。数据段在可执行文件中，也占用内存空间。
  2. 当程序读取数据段的数据时候，系统会发生缺页故障，从而分配物理内存。当程序读取BSS段数据的时候，内核会将其转到一个全零页面，不会发生缺页故障，也不会为期分配物理内存。
  3. bss是不占用.exe文件（可执行文件）空间的，其内容由**操作系统初始化**（清零）；而data却需要占用，其内容由**程序初始化**，因此造成了上述情况。

- 代码段。代码段也称正文段或文本段，通常用于存放程序执行代码(即CPU执行的机器指令)。一般C语言执行语句都编译成机器代码保存在代码段。通常代码段是可共享的，因此频繁执行的程序只需要在内存中拥有一份拷贝即可。

- 保留区。位于虚拟地址空间的最低部分，未赋予物理地址。任何对它的引用都是非法的，用于捕捉使用空指针和小整型值指针引用内存的异常情况。



# 什么是上下文切换，操作系统是怎么做的上下文切换？

>  **CPU 寄存器和程序计数器就是 CPU 上下文，因为它们都是 CPU 在运行任何任务前，必须的依赖环境。**
>
>  - CPU 寄存器是 CPU 内置的容量小、但速度极快的内存。
>  - 程序计数器则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。

- 什么是CPU上下文切换

  就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。

- CPU 上下文切换的类型

  - 进程上下文切换

    > 系统调用过程中，并不会涉及到虚拟内存等进程用户态的资源，也不会切换进程。这跟我们通常所说的进程上下文切换是不一样的：进程上下文切换，是指从一个进程切换到另一个进程运行；而系统调用过程中一直是同一个进程在运行。

    进程的运行空间分为内核空间和用户空间。进程既可以在用户空间运行，又可以在内核空间中运行。进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。

    需要明确的是，**进程是由内核来管理和调度的，进程的切换只能发生在内核态**。进程的上下文不仅包括虚拟内存，栈，全局变量等用户资源空间，还有内核堆栈，寄存器等内核空间。因此进程在切换的时候，需要把用户态资源和内核态资源保存下来，而加载了下一个进程的内核态后，还需要刷新进程的虚拟内存和用户栈。

    进程上下文切换的场景：

    1. 时间片轮转技术下，该进程分配到的时间片耗尽，就会被系统挂起，切换到其他进程
    2. 进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。
    3. 当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。
    4. 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行
    5. 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。

  - 线程上下文切换 

    1. 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。
    2. 前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据

  - 中断上下文切换

    为了快速响应硬件的事件,中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。

    **中断上下文切换并不涉及到进程的用户态**。即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。只需要关注内核资源就行，CPU寄存器，内核堆栈，硬件中断参数啥的。



# 什么是大端字节，什么是小端字节？如何转换字节序？

大端序，小端序是计算机存储数据的两种方式。

大端字节序：高位字节在前，低位字节在后，符合人类读写数值的习惯（参考普通的十进制数）

小端字节序：低位字节在前，高位字节在后。

- 为什么要有大小端字节序？统一不行吗？

  答：①内存的低地址处存放低字节，所以在强制转换数据时不需要调整字节的内容（注解：比如把int的4字节强制转换成short的2字节时，就直接把int数据存储的前两个字节给short就行，因为其前两个字节刚好就是最低的两个字节，符合转换逻辑）；②CPU做数值运算时从内存中依顺序依次从低位到高位取数据进行运算，直到最后刷新最高位的符号位，这样的运算方式会更高效。但是大端序更符合人类的习惯，主要用在网络传输和文件存储方面，符号位在所表示的数据的内存的第一个字节中，便于快速判断数据的正负和大小。

  其各自的优点就是对方的缺点，正因为两者彼此不分伯仲，再加上一些硬件厂商的坚持，因此在多字节存储顺序上始终没有一个统一的标准

- 转换字节序

  主要是针对主机字节序和网络字节序来说的。我们用的x86架构的处理器一般都是小端序存储数据，但是网络字节序是TCP/IP中规定好的数据表示格式，RFC1700规定使用“大端”字节序为网络字节序，独立于处理器操作系统。

  在Linux网络编程中，会使用下列C标准库函数进行字节之间的转换

  ```c
  #include <arpa/inet.h>
  
  uint32_t htonl(uint32_t hostlong);		//把uint32_t类型从主机序转换到网络序
  uint16_t htons(uint16_t hostshort);		//把uint16_t类型从主机序转换到网络序
  uint32_t ntohl(uint32_t netlong);		//把uint32_t类型从网络序转换到主机序
  uint16_t ntohs(uint16_t netshort);		//把uint16_t类型从网络序转换到主机序
  ```

- 如何判断本机是大端序还是小端序？

  ```c
  int i=1;   
  char *p=(char *)&i;   
  if(*p == 1)     
      printf("小端模式"); 
  else // (*p == 0)
      printf("大端模式");
  ```



# 锁是什么？

**本质思想**

并发编程中，为了保证数据操作的一致性，操作系统引入了锁机制，为了保证临界区代码的安全。锁其实是一种思想

锁的本质就是计算机中的一块内存。当这块内存空间被赋值为1的时候表示加锁了，当被赋值为0的时候表示解锁了，当然这块内存空间在哪里并不重要。多个线程抢占一个锁，就是要把这块内存赋为1。

在单核的情况下，关闭 CPU 中断，使其不能暂停当前请求而处理其他请求，从而达到赋值“锁”对应的内存空间的目的。

在多核的情况下，使用锁总线和缓存一致性技术（详情看这里），可以实现在单一时刻，只有某个CPU里面某一个核能够赋值“锁”对应的内存空间，从而达到锁的目的。

> 锁总线和缓存一致性
>
>  随着多核时代的到来，并发操作已经成了很正常的现象，操作系统必须要有一些机制和原语，以保证某些基本操作的原子性，比如处理器需要保证读一个字节或写一个字节是原子的，那么它是如何实现的呢？有两种机制：**总线锁定、缓存一致性**。
>
> CPU 和物理内存之间的通信速度远慢于 CPU 的处理速度，所以 CPU 有自己的内部缓存，根据一些规则将内存中的数据读取到内部缓存中来，以加快频繁读取的速度。那么这样就会造成cpu寄存器中的值和内存中的值出现不匹配的现象。（比如两核，i=0，第一个核i+1，第二个核取i的时候i还是0）
>
> **总线锁定机制：**在 CPU1 要做 i++ 操作的时候，其在总线上发出一个 LOCK 信号，其他处理器就不能操作缓存了该变量内存地址的缓存，也就是阻塞了其他CPU，使该处理器可以独享此共享内存。
>
> **缓存一致性：**当某块 CPU 对缓存中的数据进行操作了之后，就通知其他 CPU 放弃储存在它们内部的缓存，或者从主内存中重新读取



# 操作系统是怎么进行进程管理的？

![img](https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/20210901111845.jpeg)

如果问到进程管理，就从进程通信，同步和死锁三大块来说！

- [进程间通信](# 进程间通信方式)
- [进程同步](#进程同步)
- [死锁](#死锁)
- [进程调度](#CPU调度算法（进程调度算法）)



# 操作系统是如何做到进程阻塞的？

- 什么是阻塞

  正在运行的进程由于提出系统服务请求（如I/O操作），但因为某种原因未得到操作系统的立即响应，或者需要从其他合作进程获得的数据尚未到达等原因，该进程只能调用阻塞原语把自己阻塞，等待相应的事件出现后才被唤醒。

  进程并不总是可以立即运行的，一方面是 CPU 资源有限，另一方面则是进程时常需要等待外部事件的发生，例如 I/O 事件、定时器事件等。

- 如何做到进程阻塞

  操作系统为了支持多任务，实现了进程调度的功能，会把进程分为“运行”和“等待”等几种状态。操作系统会分时执行各个运行状态的进程，由于速度很快，看上去就像是同时执行多个任务，这里用到了时间片轮转技术，如果在时间片结束时进程还在运行，则CPU使用权将被剥夺并分配给另一个进程。**如果进程在时间片结束前阻塞或结束，则CPU当即进行切换。**

  当CPU从一个进程跑到了别的进程之后，肯定还需要跑回来，因此就有工作队列和等待队列。

  假如现在进程 A 里跑的程序有一个对象执行了某个方法将当前进程阻塞了，内核会立刻将进程A从工作队列中移除，同时创建等待队列，并新建一个引用指向进程A。这时进程A被排在了工作队列之外，不受系统调度了，这就是我们常说的被操作系统“挂起”。这也提现了阻塞和挂起的关系。阻塞是人为安排的，让你程序走到这里阻塞。而阻塞的实现方式是系统将进程挂起。

  当这个对象受到某种“刺激”（某事件触发）之后， 操作系统将该对象等待队列上的进程重新放回到工作队列上就绪，等待时间片轮转到该进程。所以，操作系统不会去尝试运行被阻塞的进程，而是由对象去等待某种“刺激”，喜欢被动。

  （要点：时间片轮转，工作队列和等待队列）

- 进程阻塞不消耗CPU资源，但是会消耗系统资源。因此系统资源不仅包含CPU，还有内存、磁盘I/O等等



# 线程是如何实现的？

这个看《操作系统》总结的也有

实现线程主要有三种方式：（1）使用内核线程实现，（2）使用用户线程实现（3）使用用户线程加轻量级进程混合实现。

- 内核线程实现

  内核线程（Kernel-Level Thread,KLT）就是直接由操作系统内核支持的线程，内核通过操纵调度器对线程进行调度，并负责将线程的任务映射到各个处理器上。每个内核线程可以视为内核的一个分身，这种操作系统就有能力同时处理多件事情，支持多线程的内核就叫做多线程内核。

  程序一般不会直接去使用内核线程，而是去使用内核线程的一种高级接口：轻量级进程（Light Weight Process ，LWP），轻量级进程就是我们通常意义上所讲的线程，由于每个轻量级进程都由一个内核线程支持，因此只有先支持内核线程，才能有轻量级进程。

- 用户线程实现

  从广义上讲，一个线程只要不是内核线程，就可以认为是用户线程（User Thread，UT）。而狭义上的用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知线程存在的实现。用户线程的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。使用用户线程的优势在于不需要系统内核支援，劣势也在于没有系统内核的支援，所有的线程操作都需要用户程序自己处理。线程的创建、切换和调度都是需要考虑的问题，因此不叫难。

- 用户线程加轻量级进程混合实现

  线程除了依赖内核线程实现和完全由用户程序自己实现之外，还有一种将内核线程与用户线程一起使用的实现方式。在这种混合实现下，既存在用户线程，也存在轻量级进程。



# 线程调度

- 协同式调度

  使用协同式调度的多线程系统，线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知系统切换到另一个线程上。协同式多线程的最大好处是实现简单，不会有线程同步问题。缺点是线程执行时间不可控制，甚至如果一个线程编写有问题，一直不告知系统进行线程切换，那么程序就会一直阻塞在那里。

- 抢占式调度

  使用抢占式调度的多线程系统，每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。在这种实现线程调度的方式下，线程的执行时间是系统可控的，也不会有一个线程导致整个进程阻塞的问题，Java使用的线程调度方式就是抢占式调度。



# 进程同步

在多道程序环境下，当程序并发执行时候，由于资源共享和进程间相互协作的关系，同一个系统中的诸进程之间会存在一下两种相互制约的关系：

1. 间接相互制约。主要是资源共享这种情况
2. 直接相互制约。源于进程间的相互合作，例如A进程向B进程提供数据，当A缓存没数据的时候B就阻塞，A缓存满时A就阻塞。

进程同步首先要搞明白临界区

- 临界区

  许多硬件资源如打印机，磁带机等，都属于临界资源，诸进程应该采取互斥方式，实现对这种资源的共享。人们把在每个进程中访问临界资源的那段代码成为临界区，显然，若能保证诸进程互斥地进入自己的临界区，便可实现诸进程对临界资源的互斥访问。

- 同步机制遵循的原则

  1. 空闲让进，当无进程处于临界区时，表明临界资源处于空闲状态，应允许一个请求进入临界区的进程立即进入自己的临界区，以有效的利用临界资源。
  2. 忙则等待，当已有进程进入临界区时，表明临界资源正在被访问，因而其他视图进入临界区的进程必须等待，以保证对临界资源的互斥访问。
  3. 有限等待，对要求访问临界资源的进程 ，应保证在有限时限内能进入自己的临界区，以免陷入死等状态。
  4. 让权等待，当进程不能进入自己的临界区时，应立即释放处理机，以免进程陷入忙等状态。

- 进程同步实现机制

  1. 提高临界区代码执行中断的优先级

     在传统操作系统中，打断进程对临界区代码的执行只有中断请求、中断一旦被接受，系统就有可能调用其它进程进入临界区，并修改此全局数据库。所以用提高临界区中断优先级方法就可以屏蔽了其它中断，保证了临界段的执行不被打断，从而实现了互斥。

  2. 自旋锁

     内核（处理器也如此）被保持在过渡状态“旋转”，直到它获得锁，“自旋锁”由此而得名。
     自旋锁像它们所保护的数据结构一样，储存在共用内存中。为了速度和使用任何在处理器体系下提供的锁定机构，获取和释放自旋锁的代码是用汇编语言写的。

  3. 信号量机制

     跟进程同步信号量机制一样

- 经典进程同步问题

  [具体参考](#经典的进程间通信(同步)问题)
  
  1. 生产者——消费者问题
  2. 哲学家进餐问题
  3. 读者——写者问题



# **进程阻塞为什么不占用cpu资源？**

因为阻塞了得不到cpu，怎么占用CPU资源



# mmap（内存映射）

## 什么是mmap？

mmap是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read,write等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。

<img src="https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/202203181050391.png" alt="img" style="float: left;" />

## mmap的过程—原理

1. 进程启动映射过程，并在虚拟地址空间中为映射创建虚拟映射区域
2. 调用内核空间的系统调用函数mmap（不同于用户空间函数），实现文件物理地址和进程虚拟地址的一一映射关系
3. 进程发起对这片映射空间的访问，引发缺页异常，实现文件内容到物理内存（主存）的拷贝

## mmap和常规文件操作的区别

常规文件的操作如下：

1. 进程发起读文件请求。
2. 内核查找文件描述符，定位到内核已打开的文件信息，找到文件的inode。
3. 查看文件页是否在缓存中，如果存在则直接返回这片页面
4. 如果不存在，缺页中断，需要定位到该文件的磁盘地址处，将数据从磁盘复制到页缓存中，然后发起页面读写过程，将页缓存中的数据发送给用户

常规文件需要先将文件页从磁盘拷贝到页缓存中，由于页缓存处在内核空间，不能被用户进程直接寻址，所以还需要将页缓存中数据页再次拷贝到内存对应的用户空间中。这样，通过了两次数据拷贝过程，才能完成进程对文件内容的获取任务。

而使用mmap操作文件中，创建新的虚拟内存区域和建立文件磁盘地址和虚拟内存区域映射这两步，没有任何文件拷贝操作。而之后访问数据时发现内存中并无数据而发起的缺页异常过程，可以通过已经建立好的映射关系，只使用一次数据拷贝，就从磁盘中将数据传入内存的用户空间中，供进程使用。

## 匿名映射





# 共享内存的原理

共享内存可以说是最有用的进程间通信方式，也是最快的IPC形式。两个不同进程A、B共享内存的意思是，同一块物理内存被映射到进程A、B各自的进程地址空间。进程A可以即时看到进程B对共享内存中数据的更新，反之亦然。由于多个进程共享同一块内存区域，必然需要某种同步机制，互斥锁和信号量都可以。

这种 IPC 机制无需内核介入！

采用共享内存通信的一个显而易见的好处是效率高，因为进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝(用户空间buf到内核，内核把数据拷贝到内存，内存拷贝到内核，内核到用户空间)，而共享内存则只拷贝两次数据(一次从输入文件到共享内存区，另一次从共享内存区到输出文件。)

实际上，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。

**共享内存位于进程空间的什么位置？**

不同进程之间共享的内存通常为同一段物理内存。进程可以将同一段物理内存连接到他们自己的地址空间中，所有的进程都可以访问共享内存中的地址。

**共享内存段最大限制是多少？**

单个共享内存段最大字节数，一般为32M

共享内存段最大个数，一般为4096

系统中共享内存页总数默认值：2097152*4096=8GB

**共享内存不保证同步，可以使用信号量来保证共享内存同步**

> 注意共享内存和内存映射是不同的

[参考](https://www.colourso.top/linux-mmapshm/)





# 共享内存和内存映射的区别

1. 共享内存可以直接创建，内存映射需要磁盘文件（匿名映射除外）

2. 共享内存效率更高

3. 内存。

   共享内存，所有的进程操作的是同一块共享内存。

   内存映射，每个进程在自己的虚拟地址空间中有一个独立的内存。

4. 数据安全

   进程突然退出时，共享内存还存在，内存映射区消失

   运行进程的电脑死机，宕机时。在共享内存中的数据会消失。内存映射区的数据 ，由于磁盘文件中的数据还在，所以内存映射区的数据还存在。

5. 生命周期

   内存映射区：进程退出，内存映射区销毁

   共享内存：进程退出，共享内存还在，标记删除（所有的关联的进程数为0），或者关机
   如果一个进程退出，会自动和共享内存进行取消关联。



# malloc是如何实现内存管理的

[参考链接](https://jacktang816.github.io/post/mallocandfree/)

**跟brk/sbrk/mmap这三个系统调用函数有关**

<img src="https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/202203181113652.png" style="zoom:80%;float:left" />

上图是一个堆内存的分布，其实在堆内存有三段空间，第一段是映射好的，指针可以访问的；第二段是未映射的地址空间，访问这段空间会报错；第三段是无法使用的空间。其中映射好的空间和未映射的空间由一个brk指针分割。所以如果要增加实际堆的可用大小，就可以移动brk指针。

**brk和sbrk：** brk函数将break指针直接设置为某个地址，而sbrk将break指针从当前位置移动increment所指定的增量。所以我们使用sbrk获得brk的地址。

**mmap函数：**mmap函数第一种用法是映射磁盘文件到内存中；而malloc使用的mmap函数的第二种用法，即匿名映射，匿名映射不映射磁盘文件，而是向映射区申请一块内存。当申请小内存的时，malloc使用sbrk分配内存；当申请大内存时，使用mmap函数申请内存；但是这只是分配了虚拟内存，还没有映射到物理内存，当访问申请的内存时，才会因为缺页异常，内核分配物理内存。



# 经典的进程间通信(同步)问题

> P是减小
>
> V是增加

## 生产者—消费者

> **问题描述：**两个进程（生产者和消费者）共享一个公共的固定大小的缓冲区。生产者将数据放入缓冲区，消费者从缓冲区中取数据。也可以扩展成m个生产者和n个消费者。当缓冲区空的时候，消费者因为取不到数据就会睡眠，知道缓冲区有数据才会被唤醒。当缓冲区满的时候，生产者无法继续往缓冲区中添加数据，就会睡眠，当缓冲区不满的时候再唤醒。
>
> **出现的问题：**为了时刻监视缓冲区大小，需要有一个变量count来映射。但是这个变量就是映射的共享内存，生产者消费者都可以修改这个变量。由于这里面对count没有加以限制会出现竞争。比如当缓冲区为空时，count=0，消费者读取到count为0，这个时候cpu的执行权限突然转移给了生产者。然后生产者发现count=0，就会马上生产一个数据放到缓冲区中，此时count=1，接着会发送一个wakeup信号给消费者，因为由于之前count=0，生产者以为消费者进入了阻塞状态。但事实上我们知道消费者还没有进入阻塞状态，因此生产者的这个wakeup信号会丢失。接着CPU执行权限有转移到消费者这里，消费者查看自己的进程表项中存储的信息发现count=0然后进入阻塞，永远不去取数据。这个时候生产者迟早会把缓冲区填满，然后生产者也会进入阻塞，然后两个进程都在阻塞下去，出现了问题。

这个模型涉及到了两种关系：

1. 生产者和消费者之间的同步关系。当缓冲区满时，必须等消费者先行动。当缓冲区空时，必须等生产者先行动。
2. 生产者和消费者之间的互斥关系。就是刚才提到的问题，对缓冲区的操作必须与其他进程互斥才行。不然很容易死锁。

解决方案：

```c
pthread_mutex_t mutex;  
pthread_cond_t producter;  
pthread_cond_t consumer;
count = pool.size()

producer(){
    while(1){
      pthread_mutex_lock(&mutex);
      pool++;
      while(pool == full){
          pthread_cond_wait(&producter, &mutex);
      }
      pthread_cond_signal(&consumer, &mutex);
      pthread_mutex_unlock(&mutex);
    }
}
consumer(){
    while(1){
      pthread_mutex_lock(&mutex);
      pool--;
      while(pool == 0){
          pthread_cond_wait(&consumer, &mutex);
      }
      pthread_cond_signal(&producter, &mutex);
      pthread_mutex_unlock(&mutex);
    }
}
```

注意不能先执行P(mutex)再执行P(empty)，这样生产者阻塞了消费者也阻塞了。

## 哲学家就餐问题

> **问题描述：**如下图所示
>
> <img src="https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/1858706-20191207225016969-138155882.png" alt="img" style="zoom: 67%;" />
>
> 哲学家只有两件事情，吃饭和思考。每个哲学家的两边都有叉子，只有当拥有左面和右面的叉子时候才能吃饭，吃完饭后放下叉子思考。
>
> **出现问题：**一下两种情况会产生死锁。第一种情况是五位哲学家同时拿起左边的叉子，就没有人能够拿到右面的叉子造成死锁。第二种情况是拿起左边叉子，然后查看右边有没有叉子，没有就放下左边叉子，有就拿起右边叉子吃饭。这样也会造成死锁，五个人还是同时拿左边的叉子，然后放下这样会永远重复。第三种情况就是哲学家拿起左边叉子然后随机等待一段时间，有右边叉子就拿没有就放下左边叉子。但是当对于核电站中的安全系统时候，随机数不可靠。

整个模型中有五个进程，互相之间是互斥的关系。解决方法是：

1. 同时拿到两个筷子
2. 对每个哲学家的动作制定规则，避免饥饿或者死锁。

> 解决方法是设置一个互斥信号量组用于对进程之间的互斥访问chopstick=[1,1,1,1,1]
>
> ```c
> semaphore chopstick[5] = {1,1,1,1,1}; //定义信号量数组mutex[5],并初始化
> Pi(){  //i号哲学家的进程
>  do{
>      P (chopstick[i] ) ; //取左边筷子
>      P (chopstick[(i+1) %5] ) ；  //取右边篌子
>      eat;  //进餐
>      V(chopstick[i]) ; //放回左边筷子
>      V(chopstick[(i+l)%5]);  //放回右边筷子
>      think;  //思考
>  } while (1);
> }
> ```
>
> 这个算法存在以下问题：**同时就餐拿起左边的筷子会产生死锁**

改进：

> 对哲学家进程施加一些限制条件，比如至多允许四个哲学家同时进餐;仅当一个哲学家左右两边的筷子都可用时才允许他抓起筷子
>
> ```c
> semaphore chopstick[5] = {1,1,1,1,1}; //初始化信号量
> semaphore mutex=l;  //设置取筷子的信号量
> Pi(){ //i号哲学家的进程
>  do{
>      P (mutex) ; //在取筷子前获得互斥量
>      P (chopstick [i]) ; //取左边筷子
>      P (chopstick[ (i+1) %5]) ;  //取右边筷子
>      V (mutex) ; //释放取筷子的信号量
>      eat;  //进餐
>      V(chopstick[i] ) ;  //放回左边筷子
>      V(chopstick[ (i+l)%5]) ;  //放回右边筷子
>      think;  // 思考
>  }while(1);
> }
> ```

## 读写问题

> **问题描述：**有读者和写者**两组**并发进程，共享一个文件，当两个或以上的读进程同时访问共享数据时不会产生副作用，但是如果有写进程在修改数据库的时候不能有其他读进程或写进程访问数据库。因此要求：①允许多个读者可以同时对文件执行读操作；②只允许一个写者往文件中写信息；③任一写者在完成写操作之前不允许其他读者或写者工作；④写者执行写操作前，应让已有的读者和写者全部退出。

问题中的三个关系：

1. 读者和写者是互斥关系
2. 写者和写者是互斥关系
3. 读者和读者不存在互斥关系

> **分析：**
>
> 1. 写者好分析，他和任意进程互斥，用互斥信号量操作就行
> 2. 读者问题比较复杂。读者必须实现和写者的互斥，而且要实现和其他读者的同步。
>
> 因此用到一个计数器，判断当前有多少读者在读文件。
>
> 当有读者的时候不能写入文件，当没有读者的时候写者才会写入文件。
>
> 同时计数器也是公共内存，对计数器的访问也应该是互斥的
>
> ```c
> int count=0;  //用于记录当前的读者数量
> semaphore mutex=1;  //用于保护更新count变量时的互斥
> semaphore rw=1;  //用于保证读者和写者互斥地访问文件
> 
> //可以看到writer比较简单
> writer () {  //写者进程
>     while (1){
>         P(rw); // 互斥访问共享文件
>         Writing;  //写入
>         V(rw) ;  //释放共享文件
>     }
> }
> 
> reader () {  // 读者进程
>     while(1){
>         P (mutex) ;  //互斥访问count变量
>         if (count==0)  //当第一个读进程读共享文件时
>             P(rw);  //阻止写进程写
>         count++;  //读者计数器加1
>         V (mutex) ;  //释放互斥变量count
> 
>         reading;  //读取
> 
>         P (mutex) ;  //互斥访问count变量
>         count--; //读者计数器减1
>         if (count==0)  //当最后一个读进程读完共享文件
>             V(rw) ;  //允许写进程写
>         V (mutex) ;  //释放互斥变量 count
>     }
> }
> ```
>





# 信号量和互斥量的区别？

我觉得主要区别在两方面：**第一方面是所有权的概念，第二个方面是用途。**

- 先说第一个所有权。

  解铃还须系铃人，一个锁住临界区的锁必须由上锁的线程解开，因此mutex的功能也就限制在了构造临界区上。

  对于信号量来说，任意多线程都可以对信号量执行PV操作。

- 第二个是用途

  也就是同步和互斥的用处。

  互斥很好说了，当我占有使用权的时候别人不能进入，独占式访问某段程序和内存。

  同步就是**调度线程**，即一些线程生产一些线程消费，让生产和消费线程保持合理执行顺序。因为semaphore本意是信号灯，含量了通知的意味，只要是我的信号量值大于等于1，那么就可以有线程或者进程来使用。

  『同步』这个词也可以拆开看，一侧是等待数据的『事件』或者『通知』，一侧是保护数据的 『临界区』，所以同步也即**同步+互斥**。信号量可以满足这两个功能，但是可以注意到两个功能的应用场景还是蛮大的，有 **do one thing and do it best** 的空间。linux 内核曾将 semaphore 作为同步原语，后面代码变得较难维护，刷了一把 mutex 变简单了不少还变快了，需要『通知』 的场景则替换为了 completion variable。

- 举个例子

  公司的咖啡机，互斥就是我一个人独占咖啡机，做完了咖啡然后撤走，咖啡机可以被别人使用。

  信号量是同步+互斥，有一个人不停地生产咖啡，每个人排队拿，如果有了就拿，没有了就阻塞等待。等有咖啡了唤醒阻塞的线程拿咖啡。

  所以更像是mutex+condition_variable

- 英文的解释

  > A [mutex](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Mutex)
  > is essentially the same thing as a binary semaphore and sometimes uses
  > the same basic implementation. The differences between them are in how
  > they are used. While a binary semaphore may be used as a mutex, a mutex
  > is a more specific use-case, in that only the thread that locked the
  > mutex is supposed to unlock it. This constraint makes it possible to
  > implement some additional features in mutexes:
  >
  > 1. Since only the thread that locked the mutex is supposed to unlock
  >    it, a mutex may store the id of thread that locked it and verify the
  >    same thread unlocks it.
  > 2. Mutexes may provide [priority inversion](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Priority_inversion)
  >    safety. If the mutex knows who locked it and is supposed to unlock it,
  >    it is possible to promote the priority of that thread whenever a
  >    higher-priority task starts waiting on the mutex.
  > 3. Mutexes may also provide deletion safety, where the thread holding the mutex cannot be accidentally deleted.
  > 4. Alternately, if the thread holding the mutex is deleted (perhaps due
  >    to an unrecoverable error), the mutex can be automatically released.
  > 5. A mutex may be recursive: a thread is allowed to lock it multiple times without causing a deadlock.





# CPU调度算法（进程调度算法）

进程调度算法也称 CPU 调度算法，毕竟进程是由 CPU 调度的。

调度分为两大类：抢占式调度和非抢占式调度。抢占式调度说明程序正在运行时可以被打断，把CPU让给其他进程。非抢占式调度表示一个进程正在运行当进程完成或者阻塞的时候把CPU让出来。

调度的的话有很多进程都会等待这调度，那么就有一些调度算法：

- **先来先服务调度算法**

  每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。

  这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。

  FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。

- **最短作业优先调度算法**

  优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。

  这显然对长作业不利，很容易造成一种极端现象。比如，一个长作业在就绪队列等待运行，而这个就绪队列有非常多的短作业，那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。

- **高响应比优先调度算法**

  前面的「先来先服务调度算法」和「最短作业优先调度算法」都没有很好的权衡短作业和长作业。

  每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行，「响应比优先级」的计算公式：

  优先权=（等待时间+服务时间）/ 服务时间。

- **时间片轮转调度算法**

  每个进程被分配一个时间段，称为时间片（\*Quantum\*），即允许该进程在该时间段中运行。

  如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；通常时间片设为 `20ms~50ms` 通常是一个比较合理的折中值。

- **最高优先级调度算法**

  对于多用户计算机系统就有不同的看法了，它们希望调度是有优先级的，即希望调度程序能从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（Highest Priority First，HPF）调度算法。

  非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。

  抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。

- **多级反馈队列调度算法**

  多级反馈队列（Multilevel Feedback Queue）调度算法是「时间片轮转算法」和「最高优先级算法」的综合和发展。

  多级表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。

  反馈表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；

  <img src="https://cdn.jsdelivr.net/gh/luogou/cloudimg/data/202203181732936.jpeg" alt="多级反馈队列" style="zoom: 67%;float:left" />

  其工作流程如下：

  1. 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从高到低，同时优先级越高时间片越短；
  2. 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；
  3. 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；



# 页面置换算法

**缺页中断：**一个进程所有地址空间里的页面不必全部常驻内存，在执行一条指令时，如果发现他要访问的页没有在内存中（即存在位为0），那么停止该指令的执行，并产生一个页不存在的异常，对应的故障处理程序可通过从物理内存加载该页的方法来排除故障，之后，原先引起的异常的指令就可以继续执行，而不再产生异常。

- 最佳页面置换算法（*OPT*）

  最佳页面置换算法基本思路是，置换在「未来」最长时间不访问的页面。所以，该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。这很理想，但是实际系统中无法实现，因为程序访问页面时是动态的，我们是无法预知每个页面在「下一次」访问前的等待时间。所以，最佳页面置换算法作用是为了衡量你的算法的效率，你的算法效率越接近该算法的效率，那么说明你的算法是高效的。

- 先进先出置换算法（*FIFO*）

  选择在内存驻留时间很长的页面进行中置换

- 最近最久未使用的置换算法（*LRU*）

  发生缺页时，选择最长时间没有被访问的页面进行置换，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。

  虽然 LRU 在理论上是可以实现的，但代价很高。为了完全实现 LRU，需要在内存中维护一个所有页面的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。困难的是，在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。所以，LRU 虽然看上去不错，但是由于开销比较大，实际应用中比较少使用。

- 时钟页面置换算法（*Lock*）

  把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。

  当发生缺页中断时，算法首先检查表针指向的页面：

  - 如果它的访问位位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
  - 如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；

- 最不常用置换算法（*LFU*）

  当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰。

  要增加一个计数器来实现，这个硬件成本是比较高的，另外如果要对这个计数器查找哪个页面访问次数最小，查找链表本身，如果链表长度很大，是非常耗时的，效率不高。



> 



# 写时拷贝底层原理

如果有多个调用者同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。

不同进程的 `虚拟内存地址` 映射到相同的 `物理内存地址`，那么就实现了共享内存的机制。

创建子进程时，将父进程的 `虚拟内存` 与 `物理内存` 映射关系复制到子进程中，并将内存设置为只读

当子进程或者父进程对内存数据进行修改时，便会触发 `写时复制` 机制：将原来的内存页复制一份新的，并重新设置其内存映射关系，将父子进程的内存读写权限设置为可读写。



# 栈区分配内存快还是堆区分配内存快 ？

[参考链接](https://mp.weixin.qq.com/s/6I5TTh7zJ4NAn8YALUEtuw)

毫无疑问，显然从栈上分配内存更快，因为从栈上分配内存仅仅就是栈指针的移动而已

在堆区上申请与释放内存是一个相对复杂的过程，因为堆本身是需要程序员(内存分配器实现者)自己管理的，而栈是编译器来维护的，堆区的维护同样涉及内存的分配与释放，但这里的内存分配与释放显然不会像栈区那样简单，一句话，这里是**按需进行内存的分配与释放**，**本质在于堆区中每一块被分配出去的内存其生命周期都不一样**，这是由程序员决定的，我倾向于把内存动态分配释放想象成去停车场找停车位。



# 申请内存时底层发生了什么？

[参考链接](https://mp.weixin.qq.com/s/0g3sS63yM2qbBja-blw5Dw)



# Cache是什么？

[参考链接](https://blog.csdn.net/wyttRain/article/details/110925923)

Cache存储器，是位于CPU和主存储器DRAM之间的一块高速缓冲存储器，规模较小，但是速度很快，通常由SRAM（静态存储器）组成。Cache的功能是提高CPU数据输入输出的速率。Cache容量小但速度快，内存速度较低但容量大，通过优化调度算法，可以让系统的性能大大改善，感觉就像是又有了主存储器的内存，又有了Cache的访问速度。



# 内存屏障

[参考链接](https://blog.csdn.net/wyttRain/article/details/114520547?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_antiscanv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_antiscanv2&utm_relevant_index=2)

现在大多数现代计算机为了提高性能而采取乱序执行，这可能会导致程序运行不符合我们预期，内存屏障就是一类同步屏障指令，是CPU或者编译器在对内存随机访问的操作中的一个同步点，只有在此点之前的所有读写操作都执行后才可以执行此点之后的操作。



# fork复制内部，为什么fork返回0？

[参考链接](https://juejin.cn/post/6909074860761169933)

fork是一个宏，内部关键指令就是0x80中断，会进入system_cal这个函数，然后有一个sys_cal_table。这个表就是一个系统调用函数表，存的都是函数指针，根据索引找到sys_fork这个函数，能找到这个函数就是因为由寄存器eax传过来个值2，索引为2的函数指针就是sys_fork。sys_fork实现调用了do_fork，do_Fork()里面有一个copy_process()。事实上fork就是实现了sys_fork。

**为什么子进程先运行：**

do_fork()第一步是调用copy_process函数来复制一个进程，并对相应的标志位等进行。设置如果copy_process调用成功的话，那么系统会有意让新开辟的进程运行，这是因为子进程一般都会马上调用exec()函数来执行其他的任务，这样就可以避免写是复制造成的开销，或者从另一个角度说，如果其首先执行父进程，而父进程在执行的过程中，可能会向地址空间中写入数据，那么这个时候，系统就会为子进程拷贝父进程原来的数据，而当子进程调用的时候，其紧接着执行拉exec()操作，那么此时，系统又会为子进程拷贝新的数据，这样的话，相比优先执行子程序，就进行了一次“多余”的拷贝。

[参考链接](https://www.zhihu.com/question/59296096)该连接中说在多核是可以同时执行的，单核的话不确定，但是cow来说确实应该子进程先执行。

我们可以通过fork返回的值来判断当前进程是子进程还是父进程。通俗的解释，可以这样看待：“其实就相当于链表，进程形成了链表，父进程的fork函数返回的值指向子进程的进程id, 因为子进程没有子进程，所以其fork函数返回的值为0.



# 锁的开销

[参考](https://www.cnblogs.com/cposture/p/10761396.html)

**所有锁的本质：**

我们针对的是多线程环境下的锁机制，基于linux做测试。每种编程语言提供的锁机制都不太一样，不过无论如何，最终都会落实到两种机制上，**一是处理器提供的原子操作指令（现在一般是CAS—compare and swap），处理器会用轮询的方式试图获得锁，在处理器（包括多核）架构里这是必不可少的机制；二是内核提供的锁系统调用，在被锁住的时候会把当前线程置于睡眠（阻塞）状态。**

实际上我们在编程的时候并不会直接调用这两种机制，而是使用编程语言所带函数库里的锁方法，锁方法内部混合使用这两种机制。以pthread库（NPTL）的pthread_mutex来举例，一把锁本质上只是一个int类型的变量，占用4个字节内存并且内存边界按4字节对齐。加锁的时候先用trylock方法（内部使用的是CAS指令）来尝试获得锁，如果无法获得锁，则调用系统调用sys_futex来试图获得锁，这时候如果还不能获得锁，当前线程就会被阻塞。(**futex的知识**)

**所以很容易得到一个结论，如果锁不存在冲突，每次获得锁和释放锁的处理器开销仅仅是CAS指令的开销，在x86-64处理器上，这个开销只比一次内存访问（无cache）高一点（大概是1.3倍）。一般的电脑上一次没有缓存的内存访问大概是十几纳秒**

**测试：**

无冲突的时候：运行了 10 亿次，平摊到每次加锁/解锁操作大概是 14ns 

锁冲突的情况：运行的结果是双核机器上消耗大约3400ns，所以锁冲突的开销大概是不冲突开销的两百倍了，相差出乎意料的大。

**锁的开销**

总结：锁的开销有好几部分，分别是：线程上下文切换的开销，调度器开销（把线程从睡眠改成就绪或者把就运行态改成阻塞），还有后续上下文切换带来的缓存不命中开销，跨处理器调度的开销等等。

**锁的优化：**

从上面可以知道，真正消耗时间的不是上锁的次数，而是锁冲突的次数。减少锁冲突的次数才是提升性能的关键。使用更细粒度的锁，可以减少锁冲突。这里说的粒度包括时间和空间，比如哈希表包含一系列哈希桶，为每个桶设置一把锁，空间粒度就会小很多－－哈希值相互不冲突的访问不会导致锁冲突，这比为整个哈希表维护一把锁的冲突机率低很多。减少时间粒度也很容易理解，加锁的范围只包含必要的代码段，尽量缩短获得锁到释放锁之间的时间，最重要的是，绝对不要在锁中进行任何可能会阻塞的操作。使用读写锁也是一个很好的减少冲突的方式，读操作之间不互斥，大大减少了冲突。

假设单向链表中的插入/删除操作很少，主要操作是搜索，那么基于单一锁的方法性能会很差。在这种情况下，应该考虑使用读写锁，即 pthread_rwlock_t，这么做就允许多个线程同时搜索链表。插入和删除操作仍然会锁住整个链表。假设执行的插入和搜索操作数量差不多相同，但是删除操作很少，那么在插入期间锁住整个链表是不合适的，在这种情况下，最好允许在链表中的分离点（disjoint point）上执行并发插入，同样使用基于读写锁的方式。在两个级别上执行锁定，链表有一个读写锁，各个节点包含一个互斥锁，在插入期间，写线程在链表上建立读锁，然后继续处理。在插入数据之前，锁住要在其后添加新数据的节点，插入之后释放此节点，然后释放读写锁。删除操作在链表上建立写锁。不需要获得与节点相关的锁；互斥锁只建立在某一个操作节点之上，大大减少锁冲突的次数。

锁本身的行为也存在进一步优化的可能性，sys_futex系统调用的作用在于让被锁住的当前线程睡眠，让出处理器供其它线程使用，既然这个过程的消耗很高，也就是说如果被锁定的时间不超过这个数值的话，根本没有必要进内核加锁——释放的处理器时间还不够消耗的。sys_futex的时间消耗够跑很多次 CAS 的，也就是说，对于一个锁冲突比较频繁而且平均锁定时间比较短的系统，一个值得考虑的优化方式是先循环调用 CAS 来尝试获得锁（这个操作也被称作自旋锁），在若干次失败后再进入内核真正加锁。当然这个优化只能在多处理器的系统里起作用（得有另一个处理器来解锁，否则自旋锁无意义）。在glibc的pthread实现里，通过对pthread_mutex设置PTHREAD_MUTEX_ADAPTIVE_NP属性就可以使用这个机制。

读多写一的情况用double buffer

> 注：CAS指令是线程数据同步的原子指令。



# mutex本质，什么是futex

Futex 是 Fast Userspace Mutexes 的缩写，现在锁的机制一般使用这个，内核态和用户态的混合机制。其设计思想其实不难理解。

在传统的 Unix 系统中，System V IPC（inter process communication），如 semaphores，msgqueues，sockets 等进程间同步机制都是对一个**内核对象**操作来完成的，这个内核对象对要同步的进程都是可见的，其提供了共享的状态信息和原子操作，用来管理互斥锁并且通知阻塞的进程。当进程间要同步的时候必须要通过系统调用（如semop()）在内核中完成。比如进程A要进入临界区，先去内核查看这个对象，有没有别的进程在占用这个临界区，出临界区的时候，也去内核查看这个对象，有没有别的进程在等待进入临界区，然后根据一定的策略唤醒等待的进程。同时经研究发现，很多同步是无竞争的，即某个进程进入互斥区，到再从某个互斥区出来这段时间，常常是没有进程也要进这个互斥区或者请求同一同步变量的。但是在这种情况下，这个进程也要陷入内核去看看有没有人和它竞争，退出的时侯还要陷入内核去看看有没有进程等待在同一同步变量上，有的话需要唤醒等待的进程。这些不必要的系统调用(或者说内核陷入)造成了大量的性能开销。为了解决这个问题，Futex就应运而生。

为了解决上述这个问题，Futex 就应运而生，Futex 是一种用户态和内核态混合的同步机制。首先，同步的进程间通过 mmap 共享一段内存，futex 变量就位于这段共享的内存中且操作是原子的，当进程尝试进入互斥区或者退出互斥区的时候，先去查看共享内存中的 futex 变量，如果没有竞争发生，就不用再执行系统调用了。当通过访问 futex 变量后进程发现有竞争发生，则还是得执行系统调用去完成相应的处理（wait 或者 wake up）。简单的说，futex 就是通过在用户态的检查，（motivation）如果了解到没有竞争就不用陷入内核了，大大提高了 low-contention 时候的效率。

mutex 是在 futex 的基础上用的内存共享变量来实现的，如果共享变量建立在进程内，它就是一个线程锁，如果它建立在进程间共享内存上，那么它是一个进程锁。pthread_mutex_t 中的 `_lock` 字段用于标记占用情况，先使用CAS判断`_lock`是否占用，若未占用，直接返回。否则，通过`__lll_lock_wait_private` 调用`SYS_futex `系统调用迫使线程进入沉睡。 CAS是用户态的 CPU 指令，若无竞争，简单修改锁状态即返回，非常高效，只有发现竞争，才通过系统调用陷入内核态。所以，FUTEX是一种用户态和内核态混合的同步机制，它保证了低竞争情况下的锁获取效率。

# 对缺页中断的处理

1. 硬件陷入内核，在堆栈中保存程序计数器。大多数机器将当前的指令，各种状态信息保存在特殊的CPU寄存器中。

2. 启动一个汇编代码保存通用寄存器和其他易失信息，防止被操作系统破坏

3. 当操作系统收到缺页中断信号后，定位到需要的虚拟页面。

4. 找到发生缺页中断的虚拟地址，操作系统检查这个地址是否有效，并检查存取与保护是否一致。

   如果不一致则杀掉该进程

   如果地址有效且没有保护错误发生，系统会检查是否有空闲页框。如果没有空闲页框就执行页面置换算法淘汰一个页面。

5. 如果选择的页框对应的页面发生了修改，即为“脏页面”，需要写回磁盘，并发生一次上下文切换，挂起产生缺页中断的进程，让其他进程运行直至全部把内容写到磁盘。

6. 一旦页框是干净的，则OS会查找要发生置换的页面对应磁盘上的地址，通过磁盘操作将其装入。在装入该页面的时候，产生缺页中断的进程仍然被挂起，运行其他可运行的进程

7. 当发生磁盘中断时表明该页面已经被装入，页表已经更新可以反映其位置，页框也被标记为正常状态。

8. 恢复发生缺页中断指令以前的状态，程序计数器重新指向引起缺页中断的指令

9. 调度引发缺页中断的进程

10. 该进程恢复寄存器和其他状态信息，返回用户空间继续执行。
